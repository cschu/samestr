#!/usr/bin/env python3
"""SameStr: Shared Strains Identification in Metagenomic Samples."""


from sys import stderr
from os.path import basename, dirname, realpath, isdir, isfile
from os import environ
import logging
import logging.config
import pathlib

from samestr.utils.utilities import collect_input_files, list_group_by_basename
from samestr.utils.ooSubprocess import serialize, parallelize_async
from samestr.utils.file_mapping import spread_args_by_input_files, get_uniform_extension, \
    set_output_structure

from samestr.convert import sam2bam, concatenate_gene_files, bam2freq
from samestr.extract import ref2freq
from samestr.filter import filter_freqs
from samestr.merge import freq2freqs
from samestr.stats import aln2stats
from samestr.compare import compare
from samestr.summarize import summarize
from samestr.db import generate_db

from samestr.utils.parse_args import read_params
from samestr.utils.citations import citations


SAMESTR_DIR = dirname(realpath(__file__))
CONVERT_DIR = SAMESTR_DIR + '/convert/'
UTILS_DIR = SAMESTR_DIR + '/utils/'
environ['PATH'] += ':' + UTILS_DIR + ':' + CONVERT_DIR


def samestr(input_args):
    """Main function for SAMESTR."""
    accepted_extensions_dict = {
        'db': ['.fasta', '.fa', '.fna', '.fasta.bz2', '.fa.bz2', '.fna.bz2'],
        'convert': ['.sam', '.sam.bz2', '.bam'],
        'extract': ['.fasta', '.fa', '.fna', '.fasta.gz', '.fa.gz', '.fna.gz'],
        'merge': ['.npy', '.npy.gz', '.npz'],
        'filter': ['.npy', '.npy.gz', '.npz'],
        'stats': ['.npy', '.npy.gz', '.npz'],
        'compare': ['.npy', '.npy.gz', '.npz'],
        'summarize': ['']
    }

    ## preprocess input/output files & extensions and map to commands
    ####
    accepted_extensions = accepted_extensions_dict[input_args['command']]
    if input_args['command'] == 'db':
        input_args['input_extension'] = get_uniform_extension(
            [input_args['markers_fasta']], accepted_extensions)
        
        if input_args['db_source'] == 'MetaPhlAn' and 'markers_pkl' not in input_args:
            LOG.error('`markers-pkl` not provided. Required when using MetaPhlAn as the source database.')
            exit(0)

    elif input_args['command'] not in ('summarize'):
        input_args['input_extension'] = get_uniform_extension(
            input_args['input_files'], accepted_extensions)

        # Count input files
        file_count = len(input_args['input_files'])
        LOG.debug('Number of input files: %s', file_count)

    # check make output dir
    if not isdir(input_args['output_dir']):
        pathlib.Path(input_args['output_dir']).mkdir(exist_ok=True, parents=True)

    input_clade = input_args.get("species")

    ## process individual commands
    ####
    if input_args['command'] == 'db':

        # expand and generate db from markers/pkl files
        generate_db(input_args)

    elif input_args['command'] == 'extract':
        cmd_args = serialize(ref2freq, [input_args])

    elif input_args['command'] == 'convert':
        # input_args['input_files'], input_args['input_extension'] = collect_input_files(
        #     input_args['input_dir'],
        #     accepted_extensions,
        #     recursive=input_args["recursive_input"]
        # )
        
        # TODO: is this still needed?
        input_args['input_files'] = list_group_by_basename(
            input_args['input_files'],
            cut_name_endings=[input_args['input_extension']])

        # Spread args over files/file-pairs, Set output names and check/make their dir
        input_args['input_sequence_type'] = 'single'
        cmd_args = spread_args_by_input_files(input_args)
        cmd_args = set_output_structure(cmd_args)

        # Run: convert
        cmd_args = parallelize_async(concatenate_gene_files, cmd_args,
                                 input_args['nprocs'])
        
        cmd_args = parallelize_async(sam2bam, cmd_args, input_args['nprocs'])        
        cmd_args = parallelize_async(bam2freq, cmd_args, input_args['nprocs'])

    elif input_args['command'] == 'merge':

        clade_file_dict = {}

        # group input files by clade
        for file_path in input_args['input_files']:
            fn = basename(file_path)
            clade = fn.split('.')[0]

            # append list if file has been merged before
            sample_names = file_path.replace(
                input_args['input_extension'], '.names.txt'
            )
            if isfile(sample_names):
                with open(sample_names, 'r', encoding="utf8") as s_n:
                    sample = s_n.read().strip().split('\n')
            else:
                sample = fn.split(input_args['input_extension'])[0].replace(f'{clade}.', '')

            # skip if not in selected clade
            if input_clade is not None and clade not in input_clade:
                continue

            clade_file_dict.setdefault(clade, []).append([sample, file_path])
            if clade not in clade_file_dict:
                clade_file_dict[clade] = [[sample, file_path]]
            else:
                clade_file_dict[clade] += [[sample, file_path]]

        cmd_args = [
               {
                    "clade": clade,
                    "input_files": files,
                    "output_dir": input_args["output_dir"],
               }
               for clade, files in clade_file_dict.items()
        ]        

        cmd_args = parallelize_async(freq2freqs, cmd_args, input_args['nprocs'])

    elif input_args['command'] in ['filter', 'stats', 'compare']:

        # For each species: group freqs with resp names
        freqs = {
            basename(fn).split(input_args['input_extension'])[0]: [fn]
            for fn in input_args['input_files']
        }

        for name in input_args['input_names']:
            clade = basename(name).split('.names.txt')[0]
            if clade not in freqs:
                LOG.warning('Skipping %s. Found name file '
                            'but no SNV profile.', clade)
            else:
                freqs[clade].append(name)

        for clade, freq in list(freqs.items()):
            if len(freq) != 2:
                LOG.warning('Skipping %s. Found SNV profile '
                            'but no name file.', clade)
                freqs.pop(clade)
            elif input_clade is not None and clade not in input_clade:
                freqs.pop(clade)

        # attach sample selection file
        if 'samples_select' in input_args:
            for i_s in input_args['samples_select']:
                clade = basename(i_s).split('.select.txt')[0]
                if clade not in freqs:
                    LOG.warning('Skipping %s. Found selection file '
                                'but no SNV profile.', clade)
                else:
                    freqs[clade].append(i_s)

        for clade, freq in list(freqs.items()):
            if len(freq) > 3:
                LOG.warning('Skipping %s. Found more than one '
                            'sample selection file.', clade)
                freqs.pop(clade)
            elif len(freq) == 2:
                freq += [None]
            elif len(freq) != 3:
                LOG.error('Unexpected number of files (%s): %s.', len(freq), clade)
                exit(0)  # <-- this is not really a zero-exit then, is it?

        # Spread args over clades
        cmd_args = []
        ignore_args = {'input_files', 'input_names', 'input_select', 'clade'}

        arg_template_d = {
            arg: value
            for arg, value in input_args.items()
            if arg not in ignore_args
        }

        for clade, (input_file, input_name, input_select) in freqs.items():
            
            args_d = {
                "input_file": input_file,
                "input_name": input_name,
                "input_select": input_select,
                "clade": clade,
            }

            args_d.update(arg_template_d)

            cmd_args.append(args_d)
            
        if input_args['command'] == 'filter':
            LOG.info('Filtering files: %s', len(freqs))

            # Run: filter
            cmd_args = parallelize_async(filter_freqs, cmd_args, input_args['nprocs'])

        elif input_args['command'] == 'stats':
            LOG.info('Gathering statistics: %s', len(freqs))

            # Run: stats
            cmd_args = parallelize_async(aln2stats, cmd_args, input_args['nprocs'])

        elif input_args['command'] == 'compare':
            LOG.info('Comparing alignments: %s', len(freqs))

            # Run: compare
            cmd_args = parallelize_async(compare, cmd_args, input_args['nprocs'])

    elif input_args['command'] == 'summarize':
        cmd_args = serialize(summarize, [input_args])

    elif input_args['command'] == 'extract':
        cmd_args = serialize(ref2freq, [input_args])



if __name__ == "__main__":
    args = read_params()

    if args['citation'] is not None:
        print(citations[args['citation']])
        exit(0)

    logging.basicConfig(
    level=getattr(logging, args['verbosity']),
    stream=stderr,
    format='%(asctime)s | %(levelname)s | %(name)s | %(funcName)s | %(lineno)d | %(message)s'
    )
    LOG = logging.getLogger(__name__)

    samestr(args)
